{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "4e3a35e9-9d9c-4ac4-9b7c-99220afa4977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from contextlib import closing\n",
    "from selenium.webdriver import Firefox # pip install selenium\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from time import sleep\n",
    "\n",
    "import pysentiment2 as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ff05bd-7fc4-4057-a849-e01b115d2d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "315d0b73-ac1e-4787-bde8-80edda9b68d0",
   "metadata": {},
   "source": [
    "# New York Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c0252564-1749-43d7-a5c5-02d1a597b498",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = 'https://www.nytimes.com/search?dropmab=true&endDate=20200423&query=Trump&sort=oldest&startDate=20200318&types=article'\n",
    "url = 'https://www.nytimes.com/search?dropmab=false&endDate=20200520&query=Trump&sort=oldest&startDate=20200423&types=article'\n",
    "\n",
    "#url = 'https://www.dailymail.co.uk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "3af5bbcf-540d-41d4-8610-95a57c018eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes a HTTP request to the website\n",
    "r1 = requests.get(url)\n",
    "r1.status_code\n",
    "\n",
    "# Returns the unicode content of the request response\n",
    "coverpage = r1.content\n",
    "\n",
    "# Soup creation\n",
    "soup1 = BeautifulSoup(coverpage, 'html5lib')\n",
    "\n",
    "# News identification\n",
    "# coverpage_news = soup1.find_all('h2', class_='linkro-darkred')\n",
    "# len(coverpage_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fc373521-e0f3-4992-8220-1382238941ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping the first 5 articles\n",
    "number_of_articles = 5# Empty lists for content, links and titles\n",
    "news_contents = []\n",
    "list_links = []\n",
    "list_titles = []\n",
    "\n",
    "for n in np.arange(0, number_of_articles):\n",
    "    \n",
    "    # only news articles (there are also albums and other things)\n",
    "    if \"inenglish\" not in coverpage_news[n].find('a')['href']:  \n",
    "        continue\n",
    "    \n",
    "    # Getting the link of the article\n",
    "    link = coverpage_news[n].find('a')['href']\n",
    "    list_links.append(link)\n",
    "    \n",
    "    # Getting the title\n",
    "    title = coverpage_news[n].find('a').get_text()\n",
    "    list_titles.append(title)\n",
    "    \n",
    "    # Reading the content (it is divided in paragraphs)\n",
    "    article = requests.get(link)\n",
    "    article_content = article.content\n",
    "    soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "    body = soup_article.find_all('div', class_='articulo-cuerpo')\n",
    "    x = body[0].find_all('p')\n",
    "    \n",
    "    # Unifying the paragraphs\n",
    "    list_paragraphs = []\n",
    "    for p in np.arange(0, len(x)):\n",
    "        paragraph = x[p].get_text()\n",
    "        list_paragraphs.append(paragraph)\n",
    "        final_article = \" \".join(list_paragraphs)\n",
    "        \n",
    "    news_contents.append(final_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f101b77c-b052-4e10-b67b-8c9b506d97c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data-testid': 'search-show-more-button', 'type': 'button'}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup1.find('button', text='Show More').attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "4cce01a8-18e4-4ead-8237-979210d4fc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_the_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f34c62c-7801-4159-9369-ad590f2f94bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links = soup1.find_all('a')\n",
    "articles = []\n",
    "for a in all_links:\n",
    "    if a.find('h4'):\n",
    "        articles.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c3c0c911-80fb-4d79-91df-7794a19ad115",
   "metadata": {},
   "outputs": [],
   "source": [
    "link = 'http://www.nytimes.com'+articles[0]['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c1a8597d-8d39-4fc7-b765-b7282af0a1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = requests.get(link)\n",
    "article_content = article.content\n",
    "soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "body = soup_article.find_all('div', class_='articulo-cuerpo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c9285bf7-331a-4585-8ba2-fe0bd5f0bcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = soup_article.find_all('p')[4:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1a558034-5c89-4c01-8ab1-e5458b2bab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "body = ''\n",
    "for i in range(len(paragraphs)):\n",
    "    body += ' ' + paragraphs[i].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "f000d4df-5d9c-46a2-95e4-5b07abd32f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1be21cea-5a36-4197-9889-629f500f2130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.077, 'neu': 0.858, 'pos': 0.065, 'compound': -0.9174}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.polarity_scores(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "d01b8578-d5ab-496d-a605-34e025251e14",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pressed the Show More button a bunch\n",
      "Got all the Articles\n",
      "Scraping...\n",
      "[==================>-] Analyzed the articles\n"
     ]
    }
   ],
   "source": [
    "#Period 1\n",
    "url = 'https://www.nytimes.com/search?dropmab=true&endDate=20200423&query=Trump&sort=oldest&startDate=20200318&types=article'\n",
    "\n",
    "with closing(Firefox()) as driver:\n",
    "    driver.get(url)\n",
    "    soup = BeautifulSoup(driver.page_source)\n",
    "    while True:#soup.find('button', text='Show More') is not None:\n",
    "        try:\n",
    "            button = driver.find_element(By.XPATH, '//button[text()=\"Show More\"]')\n",
    "        except:\n",
    "            break\n",
    "        button.click()\n",
    "        WebDriverWait(driver, 10)\n",
    "        # store it to string variable\n",
    "        soup = BeautifulSoup(driver.page_source)\n",
    "    page_source = driver.page_source\n",
    "print(\"Pressed the Show More button a bunch\")    \n",
    "\n",
    "\n",
    "del soup\n",
    "soup1 = BeautifulSoup(page_source, 'html5lib')\n",
    "all_links = soup1.find_all('a')\n",
    "articles = []\n",
    "for a in all_links:\n",
    "    if a.find('h4'):\n",
    "        articles.append(a)\n",
    "print(\"Got all the Articles\")\n",
    "\n",
    "#all_the_text = []\n",
    "scores = []\n",
    "#analyzer = SentimentIntensityAnalyzer()\n",
    "hiv4 = ps.HIV4()\n",
    "\n",
    "print('Scraping...')\n",
    "print('\\r[' + '>' + '-'*19 + ']', end=' ', flush=True)\n",
    "\n",
    "for i in range(len(articles)):   \n",
    "    link = 'http://www.nytimes.com'+articles[i]['href']\n",
    "    #print(\"Scraping \", link)\n",
    "    article = requests.get(link)\n",
    "    article_content = article.content\n",
    "    soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "    paragraphs = soup_article.find_all('p')[4:-2]\n",
    "    body = ''\n",
    "    for j in range(len(paragraphs)):\n",
    "        body += ' ' + paragraphs[j].get_text()\n",
    "    #all_the_text.append(body)\n",
    "    #if 'Covid' in body or 'Coronavirus' in body:\n",
    "       # scores.append(analyzer.polarity_scores(body))\n",
    "    tokens = hiv4.tokenize(body)\n",
    "    scores.append(hiv4.get_score(tokens))\n",
    "    temp = int(i // (len(articles)/20))\n",
    "    if temp == 0: temp = 1\n",
    "    print('\\r[' + '='*(temp-1) + '>' + '-'*(20-temp) + ']', end=' ', flush=True)\n",
    "print('\\r[' + '='*(19) + '>' + ']', end=' ', flush=True)\n",
    "print(\"Analyzed the articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "048c556e-ffef-411e-bc98-959cbe904205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "864"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#836 articles\n",
    "len(scores)\n",
    "#Compund 0.04011555023923443"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "c2a79405-890b-4159-92a5-cce873e03dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19139957327070314"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "av = 0\n",
    "for score in scores:\n",
    "    av += score['Polarity']\n",
    "av /= len(scores)\n",
    "av"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94e32b1-1925-4bbc-91ea-1674d1c12507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "bc702b80-021e-40d1-abd2-8d7a684a53d6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pressed the Show More button a bunch\n",
      "Got all the Articles\n",
      "Scraping...\n",
      "[>-------------------] Scraping...\n",
      "[===================>] Analyzed the articles\n"
     ]
    }
   ],
   "source": [
    "#Period 2\n",
    "url = 'https://www.nytimes.com/search?dropmab=false&endDate=20200520&query=Trump&sort=oldest&startDate=20200423&types=article'\n",
    "with closing(Firefox()) as driver:\n",
    "    driver.get(url)\n",
    "    soup = BeautifulSoup(driver.page_source)\n",
    "    while True:#soup.find('button', text='Show More') is not None:\n",
    "        try:\n",
    "            button = driver.find_element(By.XPATH, '//button[text()=\"Show More\"]')\n",
    "        except:\n",
    "            break\n",
    "        button.click()\n",
    "        WebDriverWait(driver, 10)\n",
    "        # store it to string variable\n",
    "        soup = BeautifulSoup(driver.page_source)\n",
    "    page_source = driver.page_source\n",
    "print(\"Pressed the Show More button a bunch\")    \n",
    "\n",
    "\n",
    "del soup\n",
    "soup1 = BeautifulSoup(page_source, 'html5lib')\n",
    "all_links = soup1.find_all('a')\n",
    "articles = []\n",
    "for a in all_links:\n",
    "    if a.find('h4'):\n",
    "        articles.append(a)\n",
    "print(\"Got all the Articles\")\n",
    "\n",
    "#all_the_text = []\n",
    "scores2 = []\n",
    "print('Scraping...')\n",
    "print('\\r[' + '>' + '-'*19 + ']', end=' ', flush=True)\n",
    "\n",
    "scores2 = []\n",
    "#analyzer = SentimentIntensityAnalyzer()\n",
    "hiv4 = ps.HIV4()\n",
    "\n",
    "print('Scraping...')\n",
    "print('\\r[' + '>' + '-'*19 + ']', end=' ', flush=True)\n",
    "\n",
    "for i in range(len(articles)):   \n",
    "    link = 'http://www.nytimes.com'+articles[i]['href']\n",
    "    #print(\"Scraping \", link)\n",
    "    article = requests.get(link)\n",
    "    article_content = article.content\n",
    "    soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "    paragraphs = soup_article.find_all('p')[4:-2]\n",
    "    body = ''\n",
    "    for j in range(len(paragraphs)):\n",
    "        body += ' ' + paragraphs[j].get_text()\n",
    "    #all_the_text.append(body)\n",
    "    #if 'Covid' in body or 'Coronavirus' in body:\n",
    "       # scores.append(analyzer.polarity_scores(body))\n",
    "    tokens = hiv4.tokenize(body)\n",
    "    scores2.append(hiv4.get_score(tokens))\n",
    "    temp = int(i // (len(articles)/20))\n",
    "    if temp == 0: temp = 1\n",
    "    print('\\r[' + '='*(temp-1) + '>' + '-'*(20-temp) + ']', end=' ', flush=True)\n",
    "print('\\r[' + '='*(19) + '>' + ']', end=' ', flush=True)\n",
    "print(\"Analyzed the articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "baeeb722-a7f4-47e0-9e4d-3375ac9671a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "692"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#642 articles\n",
    "\n",
    "len(scores2)\n",
    "#Compund 0.21758286604361396"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "36e0bac7-25ee-4ed0-b95b-bd0e9ddbc044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17382998320361828"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "av2 = 0\n",
    "for score in scores2:\n",
    "    av2 += score['Polarity']\n",
    "av2 /= len(scores2)\n",
    "av2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f59d1442-4a03-436e-9d53-514cbc0d6668",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fd88431d-5466-4dc9-816e-eff0919fca15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8799fb08-751e-44a5-80d1-a1f70194ad5b",
   "metadata": {},
   "source": [
    "# Fox News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "311b07ce-6e49-4a9d-9dee-7405bb4bdedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got all the Articles\n",
      "Scraping...\n",
      "[===================>] Analyzed the articles\n"
     ]
    }
   ],
   "source": [
    "#XPath to date-range\n",
    "url = 'https://www.foxnews.com/search-results/search?q=Trump'\n",
    "with closing(Firefox()) as driver:\n",
    "    driver.get(url)\n",
    "    \n",
    "    #choose the article content type\n",
    "    content = driver.find_element_by_xpath(\"//div[@class='filter content']/button\")\n",
    "    content.click()\n",
    "    content_article = driver.find_element_by_xpath(\"//input[@value='Article']\")\n",
    "    content_article.click()\n",
    "    \n",
    "    #we have to manually choose the date range for our search\n",
    "    min_month = driver.find_element_by_xpath(\"//div[@class='date min']/div[@class='sub month']\")\n",
    "    min_month.click()\n",
    "    min_month_choice = driver.find_element_by_xpath(\"//div[@class='date min']/div[@class='sub month']/ul[@class='option']/li[@class='03']\")\n",
    "    min_month_choice.click()\n",
    "    \n",
    "    min_day = driver.find_element_by_xpath(\"//div[@class='date min']/div[@class='sub day']\")\n",
    "    min_day.click()\n",
    "    min_day_choice = driver.find_element_by_xpath(\"//div[@class='date min']/div[@class='sub day']/ul[@class='option']/li[@class='18']\")\n",
    "    min_day_choice.click()\n",
    "    \n",
    "    min_year = driver.find_element_by_xpath(\"//div[@class='date min']/div[@class='sub year']\")\n",
    "    min_year.click()\n",
    "    min_year_choice = driver.find_element_by_xpath(\"//div[@class='date min']/div[@class='sub year']/ul[@class='option']/li[@id='2020']\")\n",
    "    min_year_choice.click()\n",
    "    \n",
    "    \n",
    "    max_month = driver.find_element_by_xpath(\"//div[@class='date max']/div[@class='sub month']\")\n",
    "    max_month.click()\n",
    "    max_month_choice = driver.find_element_by_xpath(\"//div[@class='date max']/div[@class='sub month']/ul[@class='option']/li[@class='04']\")\n",
    "    max_month_choice.click()\n",
    "    \n",
    "    max_day = driver.find_element_by_xpath(\"//div[@class='date max']/div[@class='sub day']\")\n",
    "    max_day.click()\n",
    "    max_day_choice = driver.find_element_by_xpath(\"//div[@class='date max']/div[@class='sub day']/ul[@class='option']/li[@class='22']\")\n",
    "    max_day_choice.click()\n",
    "    \n",
    "    max_year = driver.find_element_by_xpath(\"//div[@class='date max']/div[@class='sub year']\")\n",
    "    max_year.click()\n",
    "    max_year_choice = driver.find_element_by_xpath(\"//div[@class='date max']/div[@class='sub year']/ul[@class='option']/li[@id='2020']\")\n",
    "    max_year_choice.click()\n",
    "    \n",
    "    search_button = driver.find_element_by_xpath(\"//a[text()='Search']\")#\"//div[@class='search-form']/div[@class='button']\")#\n",
    "    search_button.click()\n",
    "    search_button.click()\n",
    "    WebDriverWait(driver, 10000)\n",
    "    \n",
    "    #now hit the load more button as much as possible\n",
    "    while True:\n",
    "        try:\n",
    "            button = driver.find_element_by_class_name('load-more')\n",
    "            button.click()\n",
    "            WebDriverWait(driver, 100)\n",
    "        except:\n",
    "            break\n",
    "    page_source = driver.page_source\n",
    "\n",
    "soup = BeautifulSoup(page_source, 'html5lib')\n",
    "all_articles = soup.find_all('article')\n",
    "links = []\n",
    "for a in all_articles:\n",
    "    links.append(a.find('a')['href'])\n",
    "print(\"Got all the Articles\")\n",
    "\n",
    "scores = []\n",
    "#analyzer = SentimentIntensityAnalyzer()\n",
    "hiv4 = ps.HIV4()\n",
    "\n",
    "print('Scraping...')\n",
    "print('\\r[' + '>' + '-'*19 + ']', end=' ', flush=True)\n",
    "\n",
    "for i, link in enumerate(links):   \n",
    "    article = requests.get(link)\n",
    "    article_content = article.content\n",
    "    soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "    paragraphs = soup_article.find_all('p')[3:-6]\n",
    "    body = ''\n",
    "    for j in range(len(paragraphs)):\n",
    "        body += ' ' + paragraphs[j].get_text()\n",
    "\n",
    "    body = body.replace('\\xa0', ' ')\n",
    "    #if 'Covid' in body or 'Coronavirus' in body:\n",
    "    #scores.append(analyzer.polarity_scores(body))\n",
    "    tokens = hiv4.tokenize(body)\n",
    "    scores.append(hiv4.get_score(tokens))\n",
    "    temp = int(i // (len(links)/20))\n",
    "    if temp == 0: temp = 1\n",
    "    print('\\r[' + '='*(temp-1) + '>' + '-'*(20-temp) + ']', end=' ', flush=True)\n",
    "print('\\r[' + '='*(19) + '>' + ']', end=' ', flush=True)\n",
    "print(\"Analyzed the articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "d75eff62-6721-4818-89cf-351f2550ae7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#642 articles\n",
    "\n",
    "len(scores)\n",
    "#Compund 10.833333333333334"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "55957ac1-143f-4bf0-8f51-02f39334b2e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10756928274876155"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "av2 = 0\n",
    "for score in scores:\n",
    "    av2 += score['Polarity']\n",
    "av2 /= len(scores)\n",
    "av2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "a19bacde-262f-4317-b4ff-6ccc8e62bed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got all the Articles\n",
      "Scraping...\n",
      "[===================>] Analyzed the articles\n"
     ]
    }
   ],
   "source": [
    "#XPath to date-range\n",
    "url = 'https://www.foxnews.com/search-results/search?q=Trump'\n",
    "with closing(Firefox()) as driver:\n",
    "    driver.get(url)\n",
    "    \n",
    "    #choose the article content type\n",
    "    content = driver.find_element_by_xpath(\"//div[@class='filter content']/button\")\n",
    "    content.click()\n",
    "    content_article = driver.find_element_by_xpath(\"//input[@value='Article']\")\n",
    "    content_article.click()\n",
    "    \n",
    "    #we have to manually choose the date range for our search\n",
    "    min_month = driver.find_element_by_xpath(\"//div[@class='date min']/div[@class='sub month']\")\n",
    "    min_month.click()\n",
    "    min_month_choice = driver.find_element_by_xpath(\"//div[@class='date min']/div[@class='sub month']/ul[@class='option']/li[@class='04']\")\n",
    "    min_month_choice.click()\n",
    "    \n",
    "    min_day = driver.find_element_by_xpath(\"//div[@class='date min']/div[@class='sub day']\")\n",
    "    min_day.click()\n",
    "    min_day_choice = driver.find_element_by_xpath(\"//div[@class='date min']/div[@class='sub day']/ul[@class='option']/li[@class='22']\")\n",
    "    min_day_choice.click()\n",
    "    \n",
    "    min_year = driver.find_element_by_xpath(\"//div[@class='date min']/div[@class='sub year']\")\n",
    "    min_year.click()\n",
    "    min_year_choice = driver.find_element_by_xpath(\"//div[@class='date min']/div[@class='sub year']/ul[@class='option']/li[@id='2020']\")\n",
    "    min_year_choice.click()\n",
    "    \n",
    "    \n",
    "    max_month = driver.find_element_by_xpath(\"//div[@class='date max']/div[@class='sub month']\")\n",
    "    max_month.click()\n",
    "    max_month_choice = driver.find_element_by_xpath(\"//div[@class='date max']/div[@class='sub month']/ul[@class='option']/li[@class='05']\")\n",
    "    max_month_choice.click()\n",
    "    \n",
    "    max_day = driver.find_element_by_xpath(\"//div[@class='date max']/div[@class='sub day']\")\n",
    "    max_day.click()\n",
    "    max_day_choice = driver.find_element_by_xpath(\"//div[@class='date max']/div[@class='sub day']/ul[@class='option']/li[@class='20']\")\n",
    "    max_day_choice.click()\n",
    "    \n",
    "    max_year = driver.find_element_by_xpath(\"//div[@class='date max']/div[@class='sub year']\")\n",
    "    max_year.click()\n",
    "    max_year_choice = driver.find_element_by_xpath(\"//div[@class='date max']/div[@class='sub year']/ul[@class='option']/li[@id='2020']\")\n",
    "    max_year_choice.click()\n",
    "    \n",
    "    search_button = driver.find_element_by_xpath(\"//a[text()='Search']\")#\"//div[@class='search-form']/div[@class='button']\")#\n",
    "    search_button.click()\n",
    "    search_button.click()\n",
    "    WebDriverWait(driver, 10000)\n",
    "    \n",
    "    #now hit the load more button as much as possible\n",
    "    while True:\n",
    "        try:\n",
    "            button = driver.find_element_by_class_name('load-more')\n",
    "            button.click()\n",
    "            WebDriverWait(driver, 100)\n",
    "        except:\n",
    "            break\n",
    "    page_source = driver.page_source\n",
    "\n",
    "soup = BeautifulSoup(page_source, 'html5lib')\n",
    "all_articles = soup.find_all('article')\n",
    "links = []\n",
    "for a in all_articles:\n",
    "    links.append(a.find('a')['href'])\n",
    "print(\"Got all the Articles\")\n",
    "\n",
    "scores = []\n",
    "#analyzer = SentimentIntensityAnalyzer()\n",
    "hiv4 = ps.HIV4()\n",
    "\n",
    "print('Scraping...')\n",
    "print('\\r[' + '>' + '-'*19 + ']', end=' ', flush=True)\n",
    "\n",
    "for i, link in enumerate(links):   \n",
    "    article = requests.get(link)\n",
    "    article_content = article.content\n",
    "    soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "    paragraphs = soup_article.find_all('p')[3:-6]\n",
    "    body = ''\n",
    "    for j in range(len(paragraphs)):\n",
    "        body += ' ' + paragraphs[j].get_text()\n",
    "\n",
    "    body = body.replace('\\xa0', ' ')\n",
    "    #if 'Covid' in body or 'Coronavirus' in body:\n",
    "    #scores.append(analyzer.polarity_scores(body))\n",
    "    tokens = hiv4.tokenize(body)\n",
    "    scores.append(hiv4.get_score(tokens))\n",
    "    temp = int(i // (len(links)/20))\n",
    "    if temp == 0: temp = 1\n",
    "    print('\\r[' + '='*(temp-1) + '>' + '-'*(20-temp) + ']', end=' ', flush=True)\n",
    "print('\\r[' + '='*(19) + '>' + ']', end=' ', flush=True)\n",
    "print(\"Analyzed the articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "55c824e8-1cdc-486b-b534-d4118455c524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#642 articles\n",
    "\n",
    "len(scores)\n",
    "#Compund 10.833333333333334"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "3ccdca45-aa3a-47c3-93a3-87c9366592c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15725378004509794"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "av2 = 0\n",
    "for score in scores:\n",
    "    av2 += score['Polarity']\n",
    "av2 /= len(scores)\n",
    "av2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdcdce4-3514-4f54-9071-857f2791b11b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59b2163-a9f5-4a0c-a414-3c4978610cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d01c1e4a-50e4-4583-853e-71bc68660fcb",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5e2da3-d6b8-47a1-a9e8-e70faaf053bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XPath to date-range\n",
    "url = 'https://www.cnn.com/search?q=Trump&sort=relevance&type=article&size=30&page=1'\n",
    "with closing(Firefox()) as driver:\n",
    "    driver.get(url)\n",
    "    \n",
    "    #choose the article content type\n",
    "    content = driver.find_element_by_xpath(\"//div[@class='filter content']/button\")\n",
    "    content.click()\n",
    "    content_article = driver.find_element_by_xpath(\"//input[@value='Article']\")\n",
    "    content_article.click()\n",
    "    \n",
    "    #we have to manually choose the date range for our search\n",
    "    min_month = driver.find_element_by_xpath(\"//div[@class='date min']/div[@class='sub month']\")\n",
    "    min_month.click()\n",
    "    min_month_choice = driver.find_element_by_xpath(\"//div[@class='date min']/div[@class='sub month']/ul[@class='option']/li[@class='04']\")\n",
    "    min_month_choice.click()\n",
    "    \n",
    "    min_day = driver.find_element_by_xpath(\"//div[@class='date min']/div[@class='sub day']\")\n",
    "    min_day.click()\n",
    "    min_day_choice = driver.find_element_by_xpath(\"//div[@class='date min']/div[@class='sub day']/ul[@class='option']/li[@class='22']\")\n",
    "    min_day_choice.click()\n",
    "    \n",
    "    min_year = driver.find_element_by_xpath(\"//div[@class='date min']/div[@class='sub year']\")\n",
    "    min_year.click()\n",
    "    min_year_choice = driver.find_element_by_xpath(\"//div[@class='date min']/div[@class='sub year']/ul[@class='option']/li[@id='2020']\")\n",
    "    min_year_choice.click()\n",
    "    \n",
    "    \n",
    "    max_month = driver.find_element_by_xpath(\"//div[@class='date max']/div[@class='sub month']\")\n",
    "    max_month.click()\n",
    "    max_month_choice = driver.find_element_by_xpath(\"//div[@class='date max']/div[@class='sub month']/ul[@class='option']/li[@class='05']\")\n",
    "    max_month_choice.click()\n",
    "    \n",
    "    max_day = driver.find_element_by_xpath(\"//div[@class='date max']/div[@class='sub day']\")\n",
    "    max_day.click()\n",
    "    max_day_choice = driver.find_element_by_xpath(\"//div[@class='date max']/div[@class='sub day']/ul[@class='option']/li[@class='20']\")\n",
    "    max_day_choice.click()\n",
    "    \n",
    "    max_year = driver.find_element_by_xpath(\"//div[@class='date max']/div[@class='sub year']\")\n",
    "    max_year.click()\n",
    "    max_year_choice = driver.find_element_by_xpath(\"//div[@class='date max']/div[@class='sub year']/ul[@class='option']/li[@id='2020']\")\n",
    "    max_year_choice.click()\n",
    "    \n",
    "    search_button = driver.find_element_by_xpath(\"//a[text()='Search']\")#\"//div[@class='search-form']/div[@class='button']\")#\n",
    "    search_button.click()\n",
    "    search_button.click()\n",
    "    WebDriverWait(driver, 10000)\n",
    "    \n",
    "    #now hit the load more button as much as possible\n",
    "    while True:\n",
    "        try:\n",
    "            button = driver.find_element_by_class_name('load-more')\n",
    "            button.click()\n",
    "            WebDriverWait(driver, 100)\n",
    "        except:\n",
    "            break\n",
    "    page_source = driver.page_source\n",
    "\n",
    "soup = BeautifulSoup(page_source, 'html5lib')\n",
    "all_articles = soup.find_all('article')\n",
    "links = []\n",
    "for a in all_articles:\n",
    "    links.append(a.find('a')['href'])\n",
    "print(\"Got all the Articles\")\n",
    "\n",
    "scores = []\n",
    "#analyzer = SentimentIntensityAnalyzer()\n",
    "hiv4 = ps.HIV4()\n",
    "\n",
    "print('Scraping...')\n",
    "print('\\r[' + '>' + '-'*19 + ']', end=' ', flush=True)\n",
    "\n",
    "for i, link in enumerate(links):   \n",
    "    article = requests.get(link)\n",
    "    article_content = article.content\n",
    "    soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "    paragraphs = soup_article.find_all('p')[3:-6]\n",
    "    body = ''\n",
    "    for j in range(len(paragraphs)):\n",
    "        body += ' ' + paragraphs[j].get_text()\n",
    "\n",
    "    body = body.replace('\\xa0', ' ')\n",
    "    #if 'Covid' in body or 'Coronavirus' in body:\n",
    "    #scores.append(analyzer.polarity_scores(body))\n",
    "    tokens = hiv4.tokenize(body)\n",
    "    scores.append(hiv4.get_score(tokens))\n",
    "    temp = int(i // (len(links)/20))\n",
    "    if temp == 0: temp = 1\n",
    "    print('\\r[' + '='*(temp-1) + '>' + '-'*(20-temp) + ']', end=' ', flush=True)\n",
    "print('\\r[' + '='*(19) + '>' + ']', end=' ', flush=True)\n",
    "print(\"Analyzed the articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c147a9-e629-400a-9daa-6f64b60fd91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.cnn.com/search?q=Trump&sort=relevance&type=article&size=30&from=0&page=1'\n",
    "start_date = pd.to_datetime('03/18/2020')\n",
    "end_date = pd.to_datetime('04/22/2020')\n",
    "\n",
    "body = []\n",
    "with closing(Firefox()) as driver:\n",
    "    driver.get(url)\n",
    "    \n",
    "    can_continue = True\n",
    "    current_page = 1\n",
    "    while can_continue:\n",
    "        #choose the article content type\n",
    "\n",
    "        \n",
    "        for i in range(1, 31):\n",
    "            try:\n",
    "                content = driver.find_element_by_xpath(f\"//div[@class='cnn-search__results-list']/div[{i}]\")\n",
    "                date = pd.to_datetime(content.text.split('\\n')[1])\n",
    "                if i == 1:\n",
    "                    print(content.text.split('\\n')[0])\n",
    "                if start_date < date < end_date:\n",
    "                    body.append(content.text)\n",
    "            except:\n",
    "                WebDriverWait(driver, 1000)\n",
    "                continue\n",
    "            \n",
    "        \n",
    "        try:\n",
    "            next_arrow = driver.find_element_by_xpath(\"//div[@class='pagination-arrow pagination-arrow-right cnnSearchPageLink text-active']\")\n",
    "            next_arrow.click()\n",
    "#             current_page += 1\n",
    "#             url = f'https://www.cnn.com/search?q=Trump&sort=relevance&type=article&size=30&from={30*(current_page-1)}&page={current_page}'\n",
    "#             driver.get(url)\n",
    "            WebDriverWait(driver, 15000)\n",
    "        except:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "351bb352-0489-42c6-8bc7-5bf859b517ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69d18f3-e9e8-4870-baab-8b57db939bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
